// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: google/assistant/embedded/v1alpha2/embedded_assistant.proto

package com.google.assistant.embedded.v1alpha2;

public interface AssistResponseOrBuilder extends
    // @@protoc_insertion_point(interface_extends:google.assistant.embedded.v1alpha2.AssistResponse)
    com.google.protobuf.MessageLiteOrBuilder {

  /**
   * <pre>
   * *Output-only* Indicates the type of event.
   * </pre>
   *
   * <code>optional .google.assistant.embedded.v1alpha2.AssistResponse.EventType event_type = 1;</code>
   */
  int getEventTypeValue();
  /**
   * <pre>
   * *Output-only* Indicates the type of event.
   * </pre>
   *
   * <code>optional .google.assistant.embedded.v1alpha2.AssistResponse.EventType event_type = 1;</code>
   */
  com.google.assistant.embedded.v1alpha2.AssistResponse.EventType getEventType();

  /**
   * <pre>
   * *Output-only* The audio containing the Assistant's response to the query.
   * </pre>
   *
   * <code>optional .google.assistant.embedded.v1alpha2.AudioOut audio_out = 3;</code>
   */
  boolean hasAudioOut();
  /**
   * <pre>
   * *Output-only* The audio containing the Assistant's response to the query.
   * </pre>
   *
   * <code>optional .google.assistant.embedded.v1alpha2.AudioOut audio_out = 3;</code>
   */
  com.google.assistant.embedded.v1alpha2.AudioOut getAudioOut();

  /**
   * <pre>
   * *Output-only* Contains the Assistant's visual response to the query.
   * </pre>
   *
   * <code>optional .google.assistant.embedded.v1alpha2.ScreenOut screen_out = 4;</code>
   */
  boolean hasScreenOut();
  /**
   * <pre>
   * *Output-only* Contains the Assistant's visual response to the query.
   * </pre>
   *
   * <code>optional .google.assistant.embedded.v1alpha2.ScreenOut screen_out = 4;</code>
   */
  com.google.assistant.embedded.v1alpha2.ScreenOut getScreenOut();

  /**
   * <pre>
   * *Output-only* Contains the action triggered by the query with the
   * appropriate payloads and semantic parsing.
   * </pre>
   *
   * <code>optional .google.assistant.embedded.v1alpha2.DeviceAction device_action = 6;</code>
   */
  boolean hasDeviceAction();
  /**
   * <pre>
   * *Output-only* Contains the action triggered by the query with the
   * appropriate payloads and semantic parsing.
   * </pre>
   *
   * <code>optional .google.assistant.embedded.v1alpha2.DeviceAction device_action = 6;</code>
   */
  com.google.assistant.embedded.v1alpha2.DeviceAction getDeviceAction();

  /**
   * <pre>
   * *Output-only* This repeated list contains zero or more speech recognition
   * results that correspond to consecutive portions of the audio currently
   * being processed, starting with the portion corresponding to the earliest
   * audio (and most stable portion) to the portion corresponding to the most
   * recent audio. The strings can be concatenated to view the full
   * in-progress response. When the speech recognition completes, this list
   * will contain one item with `stability` of `1.0`.
   * </pre>
   *
   * <code>repeated .google.assistant.embedded.v1alpha2.SpeechRecognitionResult speech_results = 2;</code>
   */
  java.util.List<com.google.assistant.embedded.v1alpha2.SpeechRecognitionResult> 
      getSpeechResultsList();
  /**
   * <pre>
   * *Output-only* This repeated list contains zero or more speech recognition
   * results that correspond to consecutive portions of the audio currently
   * being processed, starting with the portion corresponding to the earliest
   * audio (and most stable portion) to the portion corresponding to the most
   * recent audio. The strings can be concatenated to view the full
   * in-progress response. When the speech recognition completes, this list
   * will contain one item with `stability` of `1.0`.
   * </pre>
   *
   * <code>repeated .google.assistant.embedded.v1alpha2.SpeechRecognitionResult speech_results = 2;</code>
   */
  com.google.assistant.embedded.v1alpha2.SpeechRecognitionResult getSpeechResults(int index);
  /**
   * <pre>
   * *Output-only* This repeated list contains zero or more speech recognition
   * results that correspond to consecutive portions of the audio currently
   * being processed, starting with the portion corresponding to the earliest
   * audio (and most stable portion) to the portion corresponding to the most
   * recent audio. The strings can be concatenated to view the full
   * in-progress response. When the speech recognition completes, this list
   * will contain one item with `stability` of `1.0`.
   * </pre>
   *
   * <code>repeated .google.assistant.embedded.v1alpha2.SpeechRecognitionResult speech_results = 2;</code>
   */
  int getSpeechResultsCount();

  /**
   * <pre>
   * *Output-only* Contains output related to the user's query.
   * </pre>
   *
   * <code>optional .google.assistant.embedded.v1alpha2.DialogStateOut dialog_state_out = 5;</code>
   */
  boolean hasDialogStateOut();
  /**
   * <pre>
   * *Output-only* Contains output related to the user's query.
   * </pre>
   *
   * <code>optional .google.assistant.embedded.v1alpha2.DialogStateOut dialog_state_out = 5;</code>
   */
  com.google.assistant.embedded.v1alpha2.DialogStateOut getDialogStateOut();

  /**
   * <pre>
   * *Output-only* Debugging output that is returned if DebugConfig was set in
   * the AssistConfig
   * </pre>
   *
   * <code>optional .google.assistant.embedded.v1alpha2.DebugInfo debug_info = 8;</code>
   */
  boolean hasDebugInfo();
  /**
   * <pre>
   * *Output-only* Debugging output that is returned if DebugConfig was set in
   * the AssistConfig
   * </pre>
   *
   * <code>optional .google.assistant.embedded.v1alpha2.DebugInfo debug_info = 8;</code>
   */
  com.google.assistant.embedded.v1alpha2.DebugInfo getDebugInfo();
}
