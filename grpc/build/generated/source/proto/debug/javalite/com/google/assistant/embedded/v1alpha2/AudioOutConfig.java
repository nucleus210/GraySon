// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: google/assistant/embedded/v1alpha2/embedded_assistant.proto

package com.google.assistant.embedded.v1alpha2;

/**
 * <pre>
 * Specifies the desired format for the server to use when it returns
 * `audio_out` messages.
 * </pre>
 *
 * Protobuf type {@code google.assistant.embedded.v1alpha2.AudioOutConfig}
 */
public  final class AudioOutConfig extends
    com.google.protobuf.GeneratedMessageLite<
        AudioOutConfig, AudioOutConfig.Builder> implements
    // @@protoc_insertion_point(message_implements:google.assistant.embedded.v1alpha2.AudioOutConfig)
    AudioOutConfigOrBuilder {
  private AudioOutConfig() {
  }
  /**
   * <pre>
   * Audio encoding of the data returned in the audio message. All encodings are
   * raw audio bytes with no header, except as indicated below.
   * </pre>
   *
   * Protobuf enum {@code google.assistant.embedded.v1alpha2.AudioOutConfig.Encoding}
   */
  public enum Encoding
      implements com.google.protobuf.Internal.EnumLite {
    /**
     * <pre>
     * Not specified. Will return result [google.rpc.Code.INVALID_ARGUMENT][].
     * </pre>
     *
     * <code>ENCODING_UNSPECIFIED = 0;</code>
     */
    ENCODING_UNSPECIFIED(0),
    /**
     * <pre>
     * Uncompressed 16-bit signed little-endian samples (Linear PCM).
     * </pre>
     *
     * <code>LINEAR16 = 1;</code>
     */
    LINEAR16(1),
    /**
     * <pre>
     * MP3 audio encoding. The sample rate is encoded in the payload.
     * </pre>
     *
     * <code>MP3 = 2;</code>
     */
    MP3(2),
    /**
     * <pre>
     * Opus-encoded audio wrapped in an ogg container. The result will be a
     * file which can be played natively on Android and in some browsers (such
     * as Chrome). The quality of the encoding is considerably higher than MP3
     * while using the same bitrate. The sample rate is encoded in the payload.
     * </pre>
     *
     * <code>OPUS_IN_OGG = 3;</code>
     */
    OPUS_IN_OGG(3),
    UNRECOGNIZED(-1),
    ;

    /**
     * <pre>
     * Not specified. Will return result [google.rpc.Code.INVALID_ARGUMENT][].
     * </pre>
     *
     * <code>ENCODING_UNSPECIFIED = 0;</code>
     */
    public static final int ENCODING_UNSPECIFIED_VALUE = 0;
    /**
     * <pre>
     * Uncompressed 16-bit signed little-endian samples (Linear PCM).
     * </pre>
     *
     * <code>LINEAR16 = 1;</code>
     */
    public static final int LINEAR16_VALUE = 1;
    /**
     * <pre>
     * MP3 audio encoding. The sample rate is encoded in the payload.
     * </pre>
     *
     * <code>MP3 = 2;</code>
     */
    public static final int MP3_VALUE = 2;
    /**
     * <pre>
     * Opus-encoded audio wrapped in an ogg container. The result will be a
     * file which can be played natively on Android and in some browsers (such
     * as Chrome). The quality of the encoding is considerably higher than MP3
     * while using the same bitrate. The sample rate is encoded in the payload.
     * </pre>
     *
     * <code>OPUS_IN_OGG = 3;</code>
     */
    public static final int OPUS_IN_OGG_VALUE = 3;


    public final int getNumber() {
      return value;
    }

    /**
     * @deprecated Use {@link #forNumber(int)} instead.
     */
    @java.lang.Deprecated
    public static Encoding valueOf(int value) {
      return forNumber(value);
    }

    public static Encoding forNumber(int value) {
      switch (value) {
        case 0: return ENCODING_UNSPECIFIED;
        case 1: return LINEAR16;
        case 2: return MP3;
        case 3: return OPUS_IN_OGG;
        default: return null;
      }
    }

    public static com.google.protobuf.Internal.EnumLiteMap<Encoding>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static final com.google.protobuf.Internal.EnumLiteMap<
        Encoding> internalValueMap =
          new com.google.protobuf.Internal.EnumLiteMap<Encoding>() {
            public Encoding findValueByNumber(int number) {
              return Encoding.forNumber(number);
            }
          };

    private final int value;

    private Encoding(int value) {
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:google.assistant.embedded.v1alpha2.AudioOutConfig.Encoding)
  }

  /**
   * <pre>
   * Possible modes for audio-output on the device.
   * </pre>
   *
   * Protobuf enum {@code google.assistant.embedded.v1alpha2.AudioOutConfig.AudioMode}
   */
  public enum AudioMode
      implements com.google.protobuf.Internal.EnumLite {
    /**
     * <pre>
     * No audio mode specified.
     * The Assistant may respond as if in `AUDIO_SILENT` mode.
     * </pre>
     *
     * <code>AUDIO_MODE_UNSPECIFIED = 0;</code>
     */
    AUDIO_MODE_UNSPECIFIED(0),
    /**
     * <pre>
     * Device's audio mute mode has been activated by user.
     * The Assistant will typically not return an audio response.
     * </pre>
     *
     * <code>MUTED = 1;</code>
     */
    MUTED(1),
    /**
     * <pre>
     * No audio is currently playing, and device's audio mute mode has NOT
     * been activated by user.
     * The Assistant will typically return an audio response.
     * </pre>
     *
     * <code>SILENT = 2;</code>
     */
    SILENT(2),
    /**
     * <pre>
     * Audio is currently playing, such as a video or music.
     * The Assistant will typically not return an audio response, but may
     * for certain queries that are inherently audio (such as "what sound
     * does a cow make?" or "how do you say 'something' in Korean".
     * This mode should also typically be used when the screen is on and a
     * screen-reader is activated for accessibility, as the screen-reader
     * will read the contents of the visual response and manage navigation.
     * </pre>
     *
     * <code>PLAYING = 3;</code>
     */
    PLAYING(3),
    UNRECOGNIZED(-1),
    ;

    /**
     * <pre>
     * No audio mode specified.
     * The Assistant may respond as if in `AUDIO_SILENT` mode.
     * </pre>
     *
     * <code>AUDIO_MODE_UNSPECIFIED = 0;</code>
     */
    public static final int AUDIO_MODE_UNSPECIFIED_VALUE = 0;
    /**
     * <pre>
     * Device's audio mute mode has been activated by user.
     * The Assistant will typically not return an audio response.
     * </pre>
     *
     * <code>MUTED = 1;</code>
     */
    public static final int MUTED_VALUE = 1;
    /**
     * <pre>
     * No audio is currently playing, and device's audio mute mode has NOT
     * been activated by user.
     * The Assistant will typically return an audio response.
     * </pre>
     *
     * <code>SILENT = 2;</code>
     */
    public static final int SILENT_VALUE = 2;
    /**
     * <pre>
     * Audio is currently playing, such as a video or music.
     * The Assistant will typically not return an audio response, but may
     * for certain queries that are inherently audio (such as "what sound
     * does a cow make?" or "how do you say 'something' in Korean".
     * This mode should also typically be used when the screen is on and a
     * screen-reader is activated for accessibility, as the screen-reader
     * will read the contents of the visual response and manage navigation.
     * </pre>
     *
     * <code>PLAYING = 3;</code>
     */
    public static final int PLAYING_VALUE = 3;


    public final int getNumber() {
      return value;
    }

    /**
     * @deprecated Use {@link #forNumber(int)} instead.
     */
    @java.lang.Deprecated
    public static AudioMode valueOf(int value) {
      return forNumber(value);
    }

    public static AudioMode forNumber(int value) {
      switch (value) {
        case 0: return AUDIO_MODE_UNSPECIFIED;
        case 1: return MUTED;
        case 2: return SILENT;
        case 3: return PLAYING;
        default: return null;
      }
    }

    public static com.google.protobuf.Internal.EnumLiteMap<AudioMode>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static final com.google.protobuf.Internal.EnumLiteMap<
        AudioMode> internalValueMap =
          new com.google.protobuf.Internal.EnumLiteMap<AudioMode>() {
            public AudioMode findValueByNumber(int number) {
              return AudioMode.forNumber(number);
            }
          };

    private final int value;

    private AudioMode(int value) {
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:google.assistant.embedded.v1alpha2.AudioOutConfig.AudioMode)
  }

  public static final int ENCODING_FIELD_NUMBER = 1;
  private int encoding_;
  /**
   * <pre>
   * *Required* The encoding of audio data to be returned in all `audio_out`
   * messages.
   * </pre>
   *
   * <code>optional .google.assistant.embedded.v1alpha2.AudioOutConfig.Encoding encoding = 1;</code>
   */
  public int getEncodingValue() {
    return encoding_;
  }
  /**
   * <pre>
   * *Required* The encoding of audio data to be returned in all `audio_out`
   * messages.
   * </pre>
   *
   * <code>optional .google.assistant.embedded.v1alpha2.AudioOutConfig.Encoding encoding = 1;</code>
   */
  public com.google.assistant.embedded.v1alpha2.AudioOutConfig.Encoding getEncoding() {
    com.google.assistant.embedded.v1alpha2.AudioOutConfig.Encoding result = com.google.assistant.embedded.v1alpha2.AudioOutConfig.Encoding.forNumber(encoding_);
    return result == null ? com.google.assistant.embedded.v1alpha2.AudioOutConfig.Encoding.UNRECOGNIZED : result;
  }
  /**
   * <pre>
   * *Required* The encoding of audio data to be returned in all `audio_out`
   * messages.
   * </pre>
   *
   * <code>optional .google.assistant.embedded.v1alpha2.AudioOutConfig.Encoding encoding = 1;</code>
   */
  private void setEncodingValue(int value) {
      encoding_ = value;
  }
  /**
   * <pre>
   * *Required* The encoding of audio data to be returned in all `audio_out`
   * messages.
   * </pre>
   *
   * <code>optional .google.assistant.embedded.v1alpha2.AudioOutConfig.Encoding encoding = 1;</code>
   */
  private void setEncoding(com.google.assistant.embedded.v1alpha2.AudioOutConfig.Encoding value) {
    if (value == null) {
      throw new NullPointerException();
    }
    
    encoding_ = value.getNumber();
  }
  /**
   * <pre>
   * *Required* The encoding of audio data to be returned in all `audio_out`
   * messages.
   * </pre>
   *
   * <code>optional .google.assistant.embedded.v1alpha2.AudioOutConfig.Encoding encoding = 1;</code>
   */
  private void clearEncoding() {
    
    encoding_ = 0;
  }

  public static final int SAMPLE_RATE_HERTZ_FIELD_NUMBER = 2;
  private int sampleRateHertz_;
  /**
   * <pre>
   * *Required* The sample rate in Hertz of the audio data returned in
   * `audio_out` messages. Valid values are: 16000-24000.
   * </pre>
   *
   * <code>optional int32 sample_rate_hertz = 2;</code>
   */
  public int getSampleRateHertz() {
    return sampleRateHertz_;
  }
  /**
   * <pre>
   * *Required* The sample rate in Hertz of the audio data returned in
   * `audio_out` messages. Valid values are: 16000-24000.
   * </pre>
   *
   * <code>optional int32 sample_rate_hertz = 2;</code>
   */
  private void setSampleRateHertz(int value) {
    
    sampleRateHertz_ = value;
  }
  /**
   * <pre>
   * *Required* The sample rate in Hertz of the audio data returned in
   * `audio_out` messages. Valid values are: 16000-24000.
   * </pre>
   *
   * <code>optional int32 sample_rate_hertz = 2;</code>
   */
  private void clearSampleRateHertz() {
    
    sampleRateHertz_ = 0;
  }

  public static final int VOLUME_PERCENTAGE_FIELD_NUMBER = 3;
  private int volumePercentage_;
  /**
   * <pre>
   * *Required* Current volume setting of the device's audio output.
   * Valid values are 1 to 100 (corresponding to 1% to 100%).
   * </pre>
   *
   * <code>optional int32 volume_percentage = 3;</code>
   */
  public int getVolumePercentage() {
    return volumePercentage_;
  }
  /**
   * <pre>
   * *Required* Current volume setting of the device's audio output.
   * Valid values are 1 to 100 (corresponding to 1% to 100%).
   * </pre>
   *
   * <code>optional int32 volume_percentage = 3;</code>
   */
  private void setVolumePercentage(int value) {
    
    volumePercentage_ = value;
  }
  /**
   * <pre>
   * *Required* Current volume setting of the device's audio output.
   * Valid values are 1 to 100 (corresponding to 1% to 100%).
   * </pre>
   *
   * <code>optional int32 volume_percentage = 3;</code>
   */
  private void clearVolumePercentage() {
    
    volumePercentage_ = 0;
  }

  public static final int AUDIO_MODE_FIELD_NUMBER = 4;
  private int audioMode_;
  /**
   * <pre>
   * Current audio mode on the device while issuing the query.
   * </pre>
   *
   * <code>optional .google.assistant.embedded.v1alpha2.AudioOutConfig.AudioMode audio_mode = 4;</code>
   */
  public int getAudioModeValue() {
    return audioMode_;
  }
  /**
   * <pre>
   * Current audio mode on the device while issuing the query.
   * </pre>
   *
   * <code>optional .google.assistant.embedded.v1alpha2.AudioOutConfig.AudioMode audio_mode = 4;</code>
   */
  public com.google.assistant.embedded.v1alpha2.AudioOutConfig.AudioMode getAudioMode() {
    com.google.assistant.embedded.v1alpha2.AudioOutConfig.AudioMode result = com.google.assistant.embedded.v1alpha2.AudioOutConfig.AudioMode.forNumber(audioMode_);
    return result == null ? com.google.assistant.embedded.v1alpha2.AudioOutConfig.AudioMode.UNRECOGNIZED : result;
  }
  /**
   * <pre>
   * Current audio mode on the device while issuing the query.
   * </pre>
   *
   * <code>optional .google.assistant.embedded.v1alpha2.AudioOutConfig.AudioMode audio_mode = 4;</code>
   */
  private void setAudioModeValue(int value) {
      audioMode_ = value;
  }
  /**
   * <pre>
   * Current audio mode on the device while issuing the query.
   * </pre>
   *
   * <code>optional .google.assistant.embedded.v1alpha2.AudioOutConfig.AudioMode audio_mode = 4;</code>
   */
  private void setAudioMode(com.google.assistant.embedded.v1alpha2.AudioOutConfig.AudioMode value) {
    if (value == null) {
      throw new NullPointerException();
    }
    
    audioMode_ = value.getNumber();
  }
  /**
   * <pre>
   * Current audio mode on the device while issuing the query.
   * </pre>
   *
   * <code>optional .google.assistant.embedded.v1alpha2.AudioOutConfig.AudioMode audio_mode = 4;</code>
   */
  private void clearAudioMode() {
    
    audioMode_ = 0;
  }

  public void writeTo(com.google.protobuf.CodedOutputStream output)
                      throws java.io.IOException {
    if (encoding_ != com.google.assistant.embedded.v1alpha2.AudioOutConfig.Encoding.ENCODING_UNSPECIFIED.getNumber()) {
      output.writeEnum(1, encoding_);
    }
    if (sampleRateHertz_ != 0) {
      output.writeInt32(2, sampleRateHertz_);
    }
    if (volumePercentage_ != 0) {
      output.writeInt32(3, volumePercentage_);
    }
    if (audioMode_ != com.google.assistant.embedded.v1alpha2.AudioOutConfig.AudioMode.AUDIO_MODE_UNSPECIFIED.getNumber()) {
      output.writeEnum(4, audioMode_);
    }
  }

  public int getSerializedSize() {
    int size = memoizedSerializedSize;
    if (size != -1) return size;

    size = 0;
    if (encoding_ != com.google.assistant.embedded.v1alpha2.AudioOutConfig.Encoding.ENCODING_UNSPECIFIED.getNumber()) {
      size += com.google.protobuf.CodedOutputStream
        .computeEnumSize(1, encoding_);
    }
    if (sampleRateHertz_ != 0) {
      size += com.google.protobuf.CodedOutputStream
        .computeInt32Size(2, sampleRateHertz_);
    }
    if (volumePercentage_ != 0) {
      size += com.google.protobuf.CodedOutputStream
        .computeInt32Size(3, volumePercentage_);
    }
    if (audioMode_ != com.google.assistant.embedded.v1alpha2.AudioOutConfig.AudioMode.AUDIO_MODE_UNSPECIFIED.getNumber()) {
      size += com.google.protobuf.CodedOutputStream
        .computeEnumSize(4, audioMode_);
    }
    memoizedSerializedSize = size;
    return size;
  }

  public static com.google.assistant.embedded.v1alpha2.AudioOutConfig parseFrom(
      com.google.protobuf.ByteString data)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return com.google.protobuf.GeneratedMessageLite.parseFrom(
        DEFAULT_INSTANCE, data);
  }
  public static com.google.assistant.embedded.v1alpha2.AudioOutConfig parseFrom(
      com.google.protobuf.ByteString data,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return com.google.protobuf.GeneratedMessageLite.parseFrom(
        DEFAULT_INSTANCE, data, extensionRegistry);
  }
  public static com.google.assistant.embedded.v1alpha2.AudioOutConfig parseFrom(byte[] data)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return com.google.protobuf.GeneratedMessageLite.parseFrom(
        DEFAULT_INSTANCE, data);
  }
  public static com.google.assistant.embedded.v1alpha2.AudioOutConfig parseFrom(
      byte[] data,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return com.google.protobuf.GeneratedMessageLite.parseFrom(
        DEFAULT_INSTANCE, data, extensionRegistry);
  }
  public static com.google.assistant.embedded.v1alpha2.AudioOutConfig parseFrom(java.io.InputStream input)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageLite.parseFrom(
        DEFAULT_INSTANCE, input);
  }
  public static com.google.assistant.embedded.v1alpha2.AudioOutConfig parseFrom(
      java.io.InputStream input,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageLite.parseFrom(
        DEFAULT_INSTANCE, input, extensionRegistry);
  }
  public static com.google.assistant.embedded.v1alpha2.AudioOutConfig parseDelimitedFrom(java.io.InputStream input)
      throws java.io.IOException {
    return parseDelimitedFrom(DEFAULT_INSTANCE, input);
  }
  public static com.google.assistant.embedded.v1alpha2.AudioOutConfig parseDelimitedFrom(
      java.io.InputStream input,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws java.io.IOException {
    return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
  }
  public static com.google.assistant.embedded.v1alpha2.AudioOutConfig parseFrom(
      com.google.protobuf.CodedInputStream input)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageLite.parseFrom(
        DEFAULT_INSTANCE, input);
  }
  public static com.google.assistant.embedded.v1alpha2.AudioOutConfig parseFrom(
      com.google.protobuf.CodedInputStream input,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageLite.parseFrom(
        DEFAULT_INSTANCE, input, extensionRegistry);
  }

  public static Builder newBuilder() {
    return DEFAULT_INSTANCE.toBuilder();
  }
  public static Builder newBuilder(com.google.assistant.embedded.v1alpha2.AudioOutConfig prototype) {
    return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
  }

  /**
   * <pre>
   * Specifies the desired format for the server to use when it returns
   * `audio_out` messages.
   * </pre>
   *
   * Protobuf type {@code google.assistant.embedded.v1alpha2.AudioOutConfig}
   */
  public static final class Builder extends
      com.google.protobuf.GeneratedMessageLite.Builder<
        com.google.assistant.embedded.v1alpha2.AudioOutConfig, Builder> implements
      // @@protoc_insertion_point(builder_implements:google.assistant.embedded.v1alpha2.AudioOutConfig)
      com.google.assistant.embedded.v1alpha2.AudioOutConfigOrBuilder {
    // Construct using com.google.assistant.embedded.v1alpha2.AudioOutConfig.newBuilder()
    private Builder() {
      super(DEFAULT_INSTANCE);
    }


    /**
     * <pre>
     * *Required* The encoding of audio data to be returned in all `audio_out`
     * messages.
     * </pre>
     *
     * <code>optional .google.assistant.embedded.v1alpha2.AudioOutConfig.Encoding encoding = 1;</code>
     */
    public int getEncodingValue() {
      return instance.getEncodingValue();
    }
    /**
     * <pre>
     * *Required* The encoding of audio data to be returned in all `audio_out`
     * messages.
     * </pre>
     *
     * <code>optional .google.assistant.embedded.v1alpha2.AudioOutConfig.Encoding encoding = 1;</code>
     */
    public Builder setEncodingValue(int value) {
      copyOnWrite();
      instance.setEncodingValue(value);
      return this;
    }
    /**
     * <pre>
     * *Required* The encoding of audio data to be returned in all `audio_out`
     * messages.
     * </pre>
     *
     * <code>optional .google.assistant.embedded.v1alpha2.AudioOutConfig.Encoding encoding = 1;</code>
     */
    public com.google.assistant.embedded.v1alpha2.AudioOutConfig.Encoding getEncoding() {
      return instance.getEncoding();
    }
    /**
     * <pre>
     * *Required* The encoding of audio data to be returned in all `audio_out`
     * messages.
     * </pre>
     *
     * <code>optional .google.assistant.embedded.v1alpha2.AudioOutConfig.Encoding encoding = 1;</code>
     */
    public Builder setEncoding(com.google.assistant.embedded.v1alpha2.AudioOutConfig.Encoding value) {
      copyOnWrite();
      instance.setEncoding(value);
      return this;
    }
    /**
     * <pre>
     * *Required* The encoding of audio data to be returned in all `audio_out`
     * messages.
     * </pre>
     *
     * <code>optional .google.assistant.embedded.v1alpha2.AudioOutConfig.Encoding encoding = 1;</code>
     */
    public Builder clearEncoding() {
      copyOnWrite();
      instance.clearEncoding();
      return this;
    }

    /**
     * <pre>
     * *Required* The sample rate in Hertz of the audio data returned in
     * `audio_out` messages. Valid values are: 16000-24000.
     * </pre>
     *
     * <code>optional int32 sample_rate_hertz = 2;</code>
     */
    public int getSampleRateHertz() {
      return instance.getSampleRateHertz();
    }
    /**
     * <pre>
     * *Required* The sample rate in Hertz of the audio data returned in
     * `audio_out` messages. Valid values are: 16000-24000.
     * </pre>
     *
     * <code>optional int32 sample_rate_hertz = 2;</code>
     */
    public Builder setSampleRateHertz(int value) {
      copyOnWrite();
      instance.setSampleRateHertz(value);
      return this;
    }
    /**
     * <pre>
     * *Required* The sample rate in Hertz of the audio data returned in
     * `audio_out` messages. Valid values are: 16000-24000.
     * </pre>
     *
     * <code>optional int32 sample_rate_hertz = 2;</code>
     */
    public Builder clearSampleRateHertz() {
      copyOnWrite();
      instance.clearSampleRateHertz();
      return this;
    }

    /**
     * <pre>
     * *Required* Current volume setting of the device's audio output.
     * Valid values are 1 to 100 (corresponding to 1% to 100%).
     * </pre>
     *
     * <code>optional int32 volume_percentage = 3;</code>
     */
    public int getVolumePercentage() {
      return instance.getVolumePercentage();
    }
    /**
     * <pre>
     * *Required* Current volume setting of the device's audio output.
     * Valid values are 1 to 100 (corresponding to 1% to 100%).
     * </pre>
     *
     * <code>optional int32 volume_percentage = 3;</code>
     */
    public Builder setVolumePercentage(int value) {
      copyOnWrite();
      instance.setVolumePercentage(value);
      return this;
    }
    /**
     * <pre>
     * *Required* Current volume setting of the device's audio output.
     * Valid values are 1 to 100 (corresponding to 1% to 100%).
     * </pre>
     *
     * <code>optional int32 volume_percentage = 3;</code>
     */
    public Builder clearVolumePercentage() {
      copyOnWrite();
      instance.clearVolumePercentage();
      return this;
    }

    /**
     * <pre>
     * Current audio mode on the device while issuing the query.
     * </pre>
     *
     * <code>optional .google.assistant.embedded.v1alpha2.AudioOutConfig.AudioMode audio_mode = 4;</code>
     */
    public int getAudioModeValue() {
      return instance.getAudioModeValue();
    }
    /**
     * <pre>
     * Current audio mode on the device while issuing the query.
     * </pre>
     *
     * <code>optional .google.assistant.embedded.v1alpha2.AudioOutConfig.AudioMode audio_mode = 4;</code>
     */
    public Builder setAudioModeValue(int value) {
      copyOnWrite();
      instance.setAudioModeValue(value);
      return this;
    }
    /**
     * <pre>
     * Current audio mode on the device while issuing the query.
     * </pre>
     *
     * <code>optional .google.assistant.embedded.v1alpha2.AudioOutConfig.AudioMode audio_mode = 4;</code>
     */
    public com.google.assistant.embedded.v1alpha2.AudioOutConfig.AudioMode getAudioMode() {
      return instance.getAudioMode();
    }
    /**
     * <pre>
     * Current audio mode on the device while issuing the query.
     * </pre>
     *
     * <code>optional .google.assistant.embedded.v1alpha2.AudioOutConfig.AudioMode audio_mode = 4;</code>
     */
    public Builder setAudioMode(com.google.assistant.embedded.v1alpha2.AudioOutConfig.AudioMode value) {
      copyOnWrite();
      instance.setAudioMode(value);
      return this;
    }
    /**
     * <pre>
     * Current audio mode on the device while issuing the query.
     * </pre>
     *
     * <code>optional .google.assistant.embedded.v1alpha2.AudioOutConfig.AudioMode audio_mode = 4;</code>
     */
    public Builder clearAudioMode() {
      copyOnWrite();
      instance.clearAudioMode();
      return this;
    }

    // @@protoc_insertion_point(builder_scope:google.assistant.embedded.v1alpha2.AudioOutConfig)
  }
  protected final Object dynamicMethod(
      com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
      Object arg0, Object arg1) {
    switch (method) {
      case NEW_MUTABLE_INSTANCE: {
        return new com.google.assistant.embedded.v1alpha2.AudioOutConfig();
      }
      case IS_INITIALIZED: {
        return DEFAULT_INSTANCE;
      }
      case MAKE_IMMUTABLE: {
        return null;
      }
      case NEW_BUILDER: {
        return new Builder();
      }
      case VISIT: {
        Visitor visitor = (Visitor) arg0;
        com.google.assistant.embedded.v1alpha2.AudioOutConfig other = (com.google.assistant.embedded.v1alpha2.AudioOutConfig) arg1;
        encoding_ = visitor.visitInt(encoding_ != 0, encoding_,    other.encoding_ != 0, other.encoding_);
        sampleRateHertz_ = visitor.visitInt(sampleRateHertz_ != 0, sampleRateHertz_,
            other.sampleRateHertz_ != 0, other.sampleRateHertz_);
        volumePercentage_ = visitor.visitInt(volumePercentage_ != 0, volumePercentage_,
            other.volumePercentage_ != 0, other.volumePercentage_);
        audioMode_ = visitor.visitInt(audioMode_ != 0, audioMode_,    other.audioMode_ != 0, other.audioMode_);
        if (visitor == com.google.protobuf.GeneratedMessageLite.MergeFromVisitor
            .INSTANCE) {
        }
        return this;
      }
      case MERGE_FROM_STREAM: {
        com.google.protobuf.CodedInputStream input =
            (com.google.protobuf.CodedInputStream) arg0;
        com.google.protobuf.ExtensionRegistryLite extensionRegistry =
            (com.google.protobuf.ExtensionRegistryLite) arg1;
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              default: {
                if (!input.skipField(tag)) {
                  done = true;
                }
                break;
              }
              case 8: {
                int rawValue = input.readEnum();

                encoding_ = rawValue;
                break;
              }
              case 16: {

                sampleRateHertz_ = input.readInt32();
                break;
              }
              case 24: {

                volumePercentage_ = input.readInt32();
                break;
              }
              case 32: {
                int rawValue = input.readEnum();

                audioMode_ = rawValue;
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw new RuntimeException(e.setUnfinishedMessage(this));
        } catch (java.io.IOException e) {
          throw new RuntimeException(
              new com.google.protobuf.InvalidProtocolBufferException(
                  e.getMessage()).setUnfinishedMessage(this));
        } finally {
        }
      }
      case GET_DEFAULT_INSTANCE: {
        return DEFAULT_INSTANCE;
      }
      case GET_PARSER: {
        if (PARSER == null) {    synchronized (com.google.assistant.embedded.v1alpha2.AudioOutConfig.class) {
            if (PARSER == null) {
              PARSER = new DefaultInstanceBasedParser(DEFAULT_INSTANCE);
            }
          }
        }
        return PARSER;
      }
    }
    throw new UnsupportedOperationException();
  }


  // @@protoc_insertion_point(class_scope:google.assistant.embedded.v1alpha2.AudioOutConfig)
  private static final com.google.assistant.embedded.v1alpha2.AudioOutConfig DEFAULT_INSTANCE;
  static {
    DEFAULT_INSTANCE = new AudioOutConfig();
    DEFAULT_INSTANCE.makeImmutable();
  }

  public static com.google.assistant.embedded.v1alpha2.AudioOutConfig getDefaultInstance() {
    return DEFAULT_INSTANCE;
  }

  private static volatile com.google.protobuf.Parser<AudioOutConfig> PARSER;

  public static com.google.protobuf.Parser<AudioOutConfig> parser() {
    return DEFAULT_INSTANCE.getParserForType();
  }
}

