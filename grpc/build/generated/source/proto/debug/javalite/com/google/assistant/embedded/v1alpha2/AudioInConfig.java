// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: google/assistant/embedded/v1alpha2/embedded_assistant.proto

package com.google.assistant.embedded.v1alpha2;

/**
 * <pre>
 * Specifies how to process the `audio_in` data that will be provided in
 * subsequent requests. For recommended settings, see the Google Assistant SDK
 * [best practices](https://developers.google.com/assistant/sdk/guides/service/python/best-practices/audio).
 * </pre>
 *
 * Protobuf type {@code google.assistant.embedded.v1alpha2.AudioInConfig}
 */
public  final class AudioInConfig extends
    com.google.protobuf.GeneratedMessageLite<
        AudioInConfig, AudioInConfig.Builder> implements
    // @@protoc_insertion_point(message_implements:google.assistant.embedded.v1alpha2.AudioInConfig)
    AudioInConfigOrBuilder {
  private AudioInConfig() {
  }
  /**
   * <pre>
   * Audio encoding of the data sent in the audio message.
   * Audio must be one-channel (mono).
   * </pre>
   *
   * Protobuf enum {@code google.assistant.embedded.v1alpha2.AudioInConfig.Encoding}
   */
  public enum Encoding
      implements com.google.protobuf.Internal.EnumLite {
    /**
     * <pre>
     * Not specified. Will return result [google.rpc.Code.INVALID_ARGUMENT][].
     * </pre>
     *
     * <code>ENCODING_UNSPECIFIED = 0;</code>
     */
    ENCODING_UNSPECIFIED(0),
    /**
     * <pre>
     * Uncompressed 16-bit signed little-endian samples (Linear PCM).
     * This encoding includes no header, only the raw audio bytes.
     * </pre>
     *
     * <code>LINEAR16 = 1;</code>
     */
    LINEAR16(1),
    /**
     * <pre>
     * [`FLAC`](https://xiph.org/flac/documentation.html) (Free Lossless Audio
     * Codec) is the recommended encoding because it is
     * lossless--therefore recognition is not compromised--and
     * requires only about half the bandwidth of `LINEAR16`. This encoding
     * includes the `FLAC` stream header followed by audio data. It supports
     * 16-bit and 24-bit samples, however, not all fields in `STREAMINFO` are
     * supported.
     * </pre>
     *
     * <code>FLAC = 2;</code>
     */
    FLAC(2),
    UNRECOGNIZED(-1),
    ;

    /**
     * <pre>
     * Not specified. Will return result [google.rpc.Code.INVALID_ARGUMENT][].
     * </pre>
     *
     * <code>ENCODING_UNSPECIFIED = 0;</code>
     */
    public static final int ENCODING_UNSPECIFIED_VALUE = 0;
    /**
     * <pre>
     * Uncompressed 16-bit signed little-endian samples (Linear PCM).
     * This encoding includes no header, only the raw audio bytes.
     * </pre>
     *
     * <code>LINEAR16 = 1;</code>
     */
    public static final int LINEAR16_VALUE = 1;
    /**
     * <pre>
     * [`FLAC`](https://xiph.org/flac/documentation.html) (Free Lossless Audio
     * Codec) is the recommended encoding because it is
     * lossless--therefore recognition is not compromised--and
     * requires only about half the bandwidth of `LINEAR16`. This encoding
     * includes the `FLAC` stream header followed by audio data. It supports
     * 16-bit and 24-bit samples, however, not all fields in `STREAMINFO` are
     * supported.
     * </pre>
     *
     * <code>FLAC = 2;</code>
     */
    public static final int FLAC_VALUE = 2;


    public final int getNumber() {
      return value;
    }

    /**
     * @deprecated Use {@link #forNumber(int)} instead.
     */
    @java.lang.Deprecated
    public static Encoding valueOf(int value) {
      return forNumber(value);
    }

    public static Encoding forNumber(int value) {
      switch (value) {
        case 0: return ENCODING_UNSPECIFIED;
        case 1: return LINEAR16;
        case 2: return FLAC;
        default: return null;
      }
    }

    public static com.google.protobuf.Internal.EnumLiteMap<Encoding>
        internalGetValueMap() {
      return internalValueMap;
    }
    private static final com.google.protobuf.Internal.EnumLiteMap<
        Encoding> internalValueMap =
          new com.google.protobuf.Internal.EnumLiteMap<Encoding>() {
            public Encoding findValueByNumber(int number) {
              return Encoding.forNumber(number);
            }
          };

    private final int value;

    private Encoding(int value) {
      this.value = value;
    }

    // @@protoc_insertion_point(enum_scope:google.assistant.embedded.v1alpha2.AudioInConfig.Encoding)
  }

  public static final int ENCODING_FIELD_NUMBER = 1;
  private int encoding_;
  /**
   * <pre>
   * *Required* Encoding of audio data sent in all `audio_in` messages.
   * </pre>
   *
   * <code>optional .google.assistant.embedded.v1alpha2.AudioInConfig.Encoding encoding = 1;</code>
   */
  public int getEncodingValue() {
    return encoding_;
  }
  /**
   * <pre>
   * *Required* Encoding of audio data sent in all `audio_in` messages.
   * </pre>
   *
   * <code>optional .google.assistant.embedded.v1alpha2.AudioInConfig.Encoding encoding = 1;</code>
   */
  public com.google.assistant.embedded.v1alpha2.AudioInConfig.Encoding getEncoding() {
    com.google.assistant.embedded.v1alpha2.AudioInConfig.Encoding result = com.google.assistant.embedded.v1alpha2.AudioInConfig.Encoding.forNumber(encoding_);
    return result == null ? com.google.assistant.embedded.v1alpha2.AudioInConfig.Encoding.UNRECOGNIZED : result;
  }
  /**
   * <pre>
   * *Required* Encoding of audio data sent in all `audio_in` messages.
   * </pre>
   *
   * <code>optional .google.assistant.embedded.v1alpha2.AudioInConfig.Encoding encoding = 1;</code>
   */
  private void setEncodingValue(int value) {
      encoding_ = value;
  }
  /**
   * <pre>
   * *Required* Encoding of audio data sent in all `audio_in` messages.
   * </pre>
   *
   * <code>optional .google.assistant.embedded.v1alpha2.AudioInConfig.Encoding encoding = 1;</code>
   */
  private void setEncoding(com.google.assistant.embedded.v1alpha2.AudioInConfig.Encoding value) {
    if (value == null) {
      throw new NullPointerException();
    }
    
    encoding_ = value.getNumber();
  }
  /**
   * <pre>
   * *Required* Encoding of audio data sent in all `audio_in` messages.
   * </pre>
   *
   * <code>optional .google.assistant.embedded.v1alpha2.AudioInConfig.Encoding encoding = 1;</code>
   */
  private void clearEncoding() {
    
    encoding_ = 0;
  }

  public static final int SAMPLE_RATE_HERTZ_FIELD_NUMBER = 2;
  private int sampleRateHertz_;
  /**
   * <pre>
   * *Required* Sample rate (in Hertz) of the audio data sent in all `audio_in`
   * messages. Valid values are from 16000-24000, but 16000 is optimal.
   * For best results, set the sampling rate of the audio source to 16000 Hz.
   * If that's not possible, use the native sample rate of the audio source
   * (instead of re-sampling).
   * </pre>
   *
   * <code>optional int32 sample_rate_hertz = 2;</code>
   */
  public int getSampleRateHertz() {
    return sampleRateHertz_;
  }
  /**
   * <pre>
   * *Required* Sample rate (in Hertz) of the audio data sent in all `audio_in`
   * messages. Valid values are from 16000-24000, but 16000 is optimal.
   * For best results, set the sampling rate of the audio source to 16000 Hz.
   * If that's not possible, use the native sample rate of the audio source
   * (instead of re-sampling).
   * </pre>
   *
   * <code>optional int32 sample_rate_hertz = 2;</code>
   */
  private void setSampleRateHertz(int value) {
    
    sampleRateHertz_ = value;
  }
  /**
   * <pre>
   * *Required* Sample rate (in Hertz) of the audio data sent in all `audio_in`
   * messages. Valid values are from 16000-24000, but 16000 is optimal.
   * For best results, set the sampling rate of the audio source to 16000 Hz.
   * If that's not possible, use the native sample rate of the audio source
   * (instead of re-sampling).
   * </pre>
   *
   * <code>optional int32 sample_rate_hertz = 2;</code>
   */
  private void clearSampleRateHertz() {
    
    sampleRateHertz_ = 0;
  }

  public void writeTo(com.google.protobuf.CodedOutputStream output)
                      throws java.io.IOException {
    if (encoding_ != com.google.assistant.embedded.v1alpha2.AudioInConfig.Encoding.ENCODING_UNSPECIFIED.getNumber()) {
      output.writeEnum(1, encoding_);
    }
    if (sampleRateHertz_ != 0) {
      output.writeInt32(2, sampleRateHertz_);
    }
  }

  public int getSerializedSize() {
    int size = memoizedSerializedSize;
    if (size != -1) return size;

    size = 0;
    if (encoding_ != com.google.assistant.embedded.v1alpha2.AudioInConfig.Encoding.ENCODING_UNSPECIFIED.getNumber()) {
      size += com.google.protobuf.CodedOutputStream
        .computeEnumSize(1, encoding_);
    }
    if (sampleRateHertz_ != 0) {
      size += com.google.protobuf.CodedOutputStream
        .computeInt32Size(2, sampleRateHertz_);
    }
    memoizedSerializedSize = size;
    return size;
  }

  public static com.google.assistant.embedded.v1alpha2.AudioInConfig parseFrom(
      com.google.protobuf.ByteString data)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return com.google.protobuf.GeneratedMessageLite.parseFrom(
        DEFAULT_INSTANCE, data);
  }
  public static com.google.assistant.embedded.v1alpha2.AudioInConfig parseFrom(
      com.google.protobuf.ByteString data,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return com.google.protobuf.GeneratedMessageLite.parseFrom(
        DEFAULT_INSTANCE, data, extensionRegistry);
  }
  public static com.google.assistant.embedded.v1alpha2.AudioInConfig parseFrom(byte[] data)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return com.google.protobuf.GeneratedMessageLite.parseFrom(
        DEFAULT_INSTANCE, data);
  }
  public static com.google.assistant.embedded.v1alpha2.AudioInConfig parseFrom(
      byte[] data,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return com.google.protobuf.GeneratedMessageLite.parseFrom(
        DEFAULT_INSTANCE, data, extensionRegistry);
  }
  public static com.google.assistant.embedded.v1alpha2.AudioInConfig parseFrom(java.io.InputStream input)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageLite.parseFrom(
        DEFAULT_INSTANCE, input);
  }
  public static com.google.assistant.embedded.v1alpha2.AudioInConfig parseFrom(
      java.io.InputStream input,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageLite.parseFrom(
        DEFAULT_INSTANCE, input, extensionRegistry);
  }
  public static com.google.assistant.embedded.v1alpha2.AudioInConfig parseDelimitedFrom(java.io.InputStream input)
      throws java.io.IOException {
    return parseDelimitedFrom(DEFAULT_INSTANCE, input);
  }
  public static com.google.assistant.embedded.v1alpha2.AudioInConfig parseDelimitedFrom(
      java.io.InputStream input,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws java.io.IOException {
    return parseDelimitedFrom(DEFAULT_INSTANCE, input, extensionRegistry);
  }
  public static com.google.assistant.embedded.v1alpha2.AudioInConfig parseFrom(
      com.google.protobuf.CodedInputStream input)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageLite.parseFrom(
        DEFAULT_INSTANCE, input);
  }
  public static com.google.assistant.embedded.v1alpha2.AudioInConfig parseFrom(
      com.google.protobuf.CodedInputStream input,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageLite.parseFrom(
        DEFAULT_INSTANCE, input, extensionRegistry);
  }

  public static Builder newBuilder() {
    return DEFAULT_INSTANCE.toBuilder();
  }
  public static Builder newBuilder(com.google.assistant.embedded.v1alpha2.AudioInConfig prototype) {
    return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
  }

  /**
   * <pre>
   * Specifies how to process the `audio_in` data that will be provided in
   * subsequent requests. For recommended settings, see the Google Assistant SDK
   * [best practices](https://developers.google.com/assistant/sdk/guides/service/python/best-practices/audio).
   * </pre>
   *
   * Protobuf type {@code google.assistant.embedded.v1alpha2.AudioInConfig}
   */
  public static final class Builder extends
      com.google.protobuf.GeneratedMessageLite.Builder<
        com.google.assistant.embedded.v1alpha2.AudioInConfig, Builder> implements
      // @@protoc_insertion_point(builder_implements:google.assistant.embedded.v1alpha2.AudioInConfig)
      com.google.assistant.embedded.v1alpha2.AudioInConfigOrBuilder {
    // Construct using com.google.assistant.embedded.v1alpha2.AudioInConfig.newBuilder()
    private Builder() {
      super(DEFAULT_INSTANCE);
    }


    /**
     * <pre>
     * *Required* Encoding of audio data sent in all `audio_in` messages.
     * </pre>
     *
     * <code>optional .google.assistant.embedded.v1alpha2.AudioInConfig.Encoding encoding = 1;</code>
     */
    public int getEncodingValue() {
      return instance.getEncodingValue();
    }
    /**
     * <pre>
     * *Required* Encoding of audio data sent in all `audio_in` messages.
     * </pre>
     *
     * <code>optional .google.assistant.embedded.v1alpha2.AudioInConfig.Encoding encoding = 1;</code>
     */
    public Builder setEncodingValue(int value) {
      copyOnWrite();
      instance.setEncodingValue(value);
      return this;
    }
    /**
     * <pre>
     * *Required* Encoding of audio data sent in all `audio_in` messages.
     * </pre>
     *
     * <code>optional .google.assistant.embedded.v1alpha2.AudioInConfig.Encoding encoding = 1;</code>
     */
    public com.google.assistant.embedded.v1alpha2.AudioInConfig.Encoding getEncoding() {
      return instance.getEncoding();
    }
    /**
     * <pre>
     * *Required* Encoding of audio data sent in all `audio_in` messages.
     * </pre>
     *
     * <code>optional .google.assistant.embedded.v1alpha2.AudioInConfig.Encoding encoding = 1;</code>
     */
    public Builder setEncoding(com.google.assistant.embedded.v1alpha2.AudioInConfig.Encoding value) {
      copyOnWrite();
      instance.setEncoding(value);
      return this;
    }
    /**
     * <pre>
     * *Required* Encoding of audio data sent in all `audio_in` messages.
     * </pre>
     *
     * <code>optional .google.assistant.embedded.v1alpha2.AudioInConfig.Encoding encoding = 1;</code>
     */
    public Builder clearEncoding() {
      copyOnWrite();
      instance.clearEncoding();
      return this;
    }

    /**
     * <pre>
     * *Required* Sample rate (in Hertz) of the audio data sent in all `audio_in`
     * messages. Valid values are from 16000-24000, but 16000 is optimal.
     * For best results, set the sampling rate of the audio source to 16000 Hz.
     * If that's not possible, use the native sample rate of the audio source
     * (instead of re-sampling).
     * </pre>
     *
     * <code>optional int32 sample_rate_hertz = 2;</code>
     */
    public int getSampleRateHertz() {
      return instance.getSampleRateHertz();
    }
    /**
     * <pre>
     * *Required* Sample rate (in Hertz) of the audio data sent in all `audio_in`
     * messages. Valid values are from 16000-24000, but 16000 is optimal.
     * For best results, set the sampling rate of the audio source to 16000 Hz.
     * If that's not possible, use the native sample rate of the audio source
     * (instead of re-sampling).
     * </pre>
     *
     * <code>optional int32 sample_rate_hertz = 2;</code>
     */
    public Builder setSampleRateHertz(int value) {
      copyOnWrite();
      instance.setSampleRateHertz(value);
      return this;
    }
    /**
     * <pre>
     * *Required* Sample rate (in Hertz) of the audio data sent in all `audio_in`
     * messages. Valid values are from 16000-24000, but 16000 is optimal.
     * For best results, set the sampling rate of the audio source to 16000 Hz.
     * If that's not possible, use the native sample rate of the audio source
     * (instead of re-sampling).
     * </pre>
     *
     * <code>optional int32 sample_rate_hertz = 2;</code>
     */
    public Builder clearSampleRateHertz() {
      copyOnWrite();
      instance.clearSampleRateHertz();
      return this;
    }

    // @@protoc_insertion_point(builder_scope:google.assistant.embedded.v1alpha2.AudioInConfig)
  }
  protected final Object dynamicMethod(
      com.google.protobuf.GeneratedMessageLite.MethodToInvoke method,
      Object arg0, Object arg1) {
    switch (method) {
      case NEW_MUTABLE_INSTANCE: {
        return new com.google.assistant.embedded.v1alpha2.AudioInConfig();
      }
      case IS_INITIALIZED: {
        return DEFAULT_INSTANCE;
      }
      case MAKE_IMMUTABLE: {
        return null;
      }
      case NEW_BUILDER: {
        return new Builder();
      }
      case VISIT: {
        Visitor visitor = (Visitor) arg0;
        com.google.assistant.embedded.v1alpha2.AudioInConfig other = (com.google.assistant.embedded.v1alpha2.AudioInConfig) arg1;
        encoding_ = visitor.visitInt(encoding_ != 0, encoding_,    other.encoding_ != 0, other.encoding_);
        sampleRateHertz_ = visitor.visitInt(sampleRateHertz_ != 0, sampleRateHertz_,
            other.sampleRateHertz_ != 0, other.sampleRateHertz_);
        if (visitor == com.google.protobuf.GeneratedMessageLite.MergeFromVisitor
            .INSTANCE) {
        }
        return this;
      }
      case MERGE_FROM_STREAM: {
        com.google.protobuf.CodedInputStream input =
            (com.google.protobuf.CodedInputStream) arg0;
        com.google.protobuf.ExtensionRegistryLite extensionRegistry =
            (com.google.protobuf.ExtensionRegistryLite) arg1;
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              default: {
                if (!input.skipField(tag)) {
                  done = true;
                }
                break;
              }
              case 8: {
                int rawValue = input.readEnum();

                encoding_ = rawValue;
                break;
              }
              case 16: {

                sampleRateHertz_ = input.readInt32();
                break;
              }
            }
          }
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw new RuntimeException(e.setUnfinishedMessage(this));
        } catch (java.io.IOException e) {
          throw new RuntimeException(
              new com.google.protobuf.InvalidProtocolBufferException(
                  e.getMessage()).setUnfinishedMessage(this));
        } finally {
        }
      }
      case GET_DEFAULT_INSTANCE: {
        return DEFAULT_INSTANCE;
      }
      case GET_PARSER: {
        if (PARSER == null) {    synchronized (com.google.assistant.embedded.v1alpha2.AudioInConfig.class) {
            if (PARSER == null) {
              PARSER = new DefaultInstanceBasedParser(DEFAULT_INSTANCE);
            }
          }
        }
        return PARSER;
      }
    }
    throw new UnsupportedOperationException();
  }


  // @@protoc_insertion_point(class_scope:google.assistant.embedded.v1alpha2.AudioInConfig)
  private static final com.google.assistant.embedded.v1alpha2.AudioInConfig DEFAULT_INSTANCE;
  static {
    DEFAULT_INSTANCE = new AudioInConfig();
    DEFAULT_INSTANCE.makeImmutable();
  }

  public static com.google.assistant.embedded.v1alpha2.AudioInConfig getDefaultInstance() {
    return DEFAULT_INSTANCE;
  }

  private static volatile com.google.protobuf.Parser<AudioInConfig> PARSER;

  public static com.google.protobuf.Parser<AudioInConfig> parser() {
    return DEFAULT_INSTANCE.getParserForType();
  }
}

